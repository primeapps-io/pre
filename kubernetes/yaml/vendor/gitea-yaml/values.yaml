## Gitea image
## ref: https://hub.docker.com/r/gitea/gitea/tags/
##

images:
  gitea: "gitea/gitea:1.9.1"
  imagePullPolicy: Always
  ## Specify imagePullSecrets
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  ##
  # imagePullSecrets: myregistrykey

## cache settings
memcached:
  maxItemMemory: 64
  verbosity: v
  extendedOptions: modern

## ingress settings - Optional
ingress:
  enabled: true
  # annotations used by the ingress - ex for k8s nginx ingress controller:
  ingress_annotations:
   kubernetes.io/ingress.class: nginx
   nginx.ingress.kubernetes.io/proxy-body-size: "0"
   nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
   nginx.ingress.kubernetes.io/proxy-send-timeout: "600"

  #tls support for the ingress
  tls:
   - secretName: primeapps-wildcard-ssl
     hosts:
       - 'git-dev.primeapps.io'

## chart defaults to using an ingress for http, but change to LoadBalancer if using you cluster supports it
service:
  http:
    serviceType: ClusterIP
    port: 3000
    #nodePort: 30280
    # sometimes if is necesary to access through an external port i.e. http(s)://<dns-name>:<external-port>
    externalPort: 8280
    externalHost: git-dev.primeapps.io
  ssh:
    serviceType: ClusterIP
    port: 22
    #nodePort: 30222
    ## if serving on a different external port used for determining the ssh url in the gui
    #externalPort: 8022
    #externalHost: git.example.com

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources:
  gitea:
    requests:
      memory: 500Mi
      cpu: 1000m
    limits:
      memory: 2Gi
      cpu: 1
  postgres:
    requests:
      memory: 200Mi
      cpu: 200m
    limits:
      memory: 2Gi
      cpu: 1
  memcached:
    requests:
      memory: 64Mi
      cpu: 50m



## Enable persistence using Persistent Volume Claims
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
## ref:
##
persistence:
  enabled: false
  #existingGiteaClaim: gitea-gitea
  #existingPostgresClaim: gitea-postgres
  giteaSize: 10Gi
  postgresSize: 5Gi
  #storageClass: glusterfs
  accessMode: ReadWriteMany
## addtional annotations for hte pvcs  uncommenting below will prevent helm from deleting the pvc when hte chart is deleted
#  annotations:
#    "helm.sh/resource-policy": keep

## if you want to mount a volume directly without using a storageClass or pvcs
#  directGiteaVolumeMount:
#    glusterfs:
#      endpoints: "192.168.1.1 192.168.1.2 192.168.1.3"
#      path: giteaData
#  directPostgresVolumeMount:
#    glusterfs:
#      endpoints: "192.168.1.1 192.168.1.2 192.168.1.3"
#      path: giteaPostgresData

#Connect to an external database
externalDB:
  dbUser: "primeapps"
  dbPassword: "pr!meAppsi0Dev"
  dbHost: "primeapps-pre-dev.cde77rccad5m.eu-west-1.rds.amazonaws.com" # or some external host
  dbPort: "5432"
  dbDatabase: "gitea"

# valid types: postgres, mysql, mssql, sqllite
dbType: "postgres"
useInPodPostgres: false

## Node labels and tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature
nodeSelector: {}
tolerations: []
affinity: {}

## Annotations for the deployment and nodes.
deploymentAnnotations: {}
podAnnotations: {}

## In order to disable initial install screen you must have secretKey and disableInstaller=true
config:
  secretKey: define_your_secret
  disableInstaller: true
  offlineMode: false
  requireSignin: false
  disableRegistration: true
  openidSignin: false

